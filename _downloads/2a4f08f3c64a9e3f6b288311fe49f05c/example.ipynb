{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Oddball Tones Example\nEstimate NCRFs for standard and oddball tones.\n\nFor this tutorial, we use the auditory Brainstorm tutorial dataset :cite:`Brainstorm` that is available as a part of the Brainstorm software.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Downloading the dataset requires answering an interactive prompt (see\n   :func:`mne.datasets.brainstorm.bst_auditory.data_path`).</p></div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Proloy Das <proloy@umd.edu>\n#          Christian Brodbeck <brodbecc@mcmaster.ca>\n#\n# sphinx_gallery_thumbnail_number = 3\n\nimport numpy as np\nimport pandas as pd\nimport eelbrain\nimport mne\nfrom ncrf import fit_ncrf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessing\nPreprocess MEG Data: low pass filtering, power line attenuation, downsampling, etc.\nWe broadly follow [this mne-python tutorial](https://mne.tools/stable/auto_tutorials/io/60_ctf_bst_auditory.html).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data_path = mne.datasets.brainstorm.bst_auditory.data_path()\nraw_fname = data_path / 'MEG' / 'bst_auditory' / 'S01_AEF_20131218_01.ds'\nraw = mne.io.read_raw_ctf(raw_fname, preload=False)\nn_times_run1 = raw.n_times\n\n# We mark a set of bad channels that seem noisier than others. \nraw.info['bads'] = ['MLO52-4408', 'MRT51-4408', 'MLO42-4408', 'MLO43-4408']\n\nannotations_df = pd.DataFrame()\noffset = n_times_run1\nfor idx in [1]:\n    csv_fname = data_path / 'MEG' / 'bst_auditory' / f'events_bad_0{idx}.csv'\n    df = pd.read_csv(csv_fname, header=None, names=['onset', 'duration', 'id', 'label'])\n    print('Events from run {0}:'.format(idx))\n    print(df)\n\n    df['onset'] += offset * (idx - 1)\n    annotations_df = pd.concat([annotations_df, df], axis=0)\n\n# Conversion from samples to times:\nonsets = annotations_df['onset'].values / raw.info['sfreq']\ndurations = annotations_df['duration'].values / raw.info['sfreq']\ndescriptions = annotations_df['label'].values\n\nannotations = mne.Annotations(onsets, durations, descriptions)\nraw.set_annotations(annotations)\ndel onsets, durations, descriptions\n\n\n# events are the presentation times of the audio stimuli: UPPT001\nevent_fname = data_path / 'MEG' / 'bst_auditory' / 'S01_AEF_20131218_01-eve.fif'\nevents = mne.find_events(raw, stim_channel='UPPT001')\n# The event timing is adjusted by comparing the trigger times on detected sound onsets on channel UADC001-4408.\nsound_data = raw[raw.ch_names.index('UADC001-4408')][0][0]\nonsets = np.where(np.abs(sound_data) > 2. * np.std(sound_data))[0]\nmin_diff = int(0.5 * raw.info['sfreq'])\ndiffs = np.concatenate([[min_diff + 1], np.diff(onsets)])\nonsets = onsets[diffs > min_diff]\nassert len(onsets) == len(events)\ndiffs = 1000. * (events[:, 0] - onsets) / raw.info['sfreq']\nprint('Trigger delay removed (\u03bc \u00b1 \u03c3): %0.1f \u00b1 %0.1f ms'\n      % (np.mean(diffs), np.std(diffs)))\n\n# events times are rescaled according to new sampling freq, 100 Hz\nevents[:, 0] = np.int64(onsets * 100 / raw.info['sfreq'])\nmne.write_events(event_fname, events, overwrite=True)\n\ndel sound_data, diffs\n\n## set EOG channel\nraw.set_eeg_reference('average', projection=True)\n# raw_AEF.plot_psd(tmax=60., average=False)\nraw.load_data()\nraw.notch_filter(np.arange(60, 181, 60), fir_design='firwin')\n\n# band pass filtering 1-8 Hz\nraw.filter(1.0, 8.0, fir_design='firwin')\n\n# resample to 100 Hz\nraw.resample(100, npad=\"auto\")\n\n### LOAD RELEVANT VARIABLES AS eelbrain.NDVar\n# load as epochs for plot only\nds = eelbrain.load.fiff.events(raw=raw, proj=True, stim_channel='UPPT001', events=event_fname)\nepochs = eelbrain.load.fiff.epochs(ds, tmin=-0.1, tmax=0.5, baseline=(None, 0))\neelbrain.plot.Butterfly(epochs)\n\n# pick MEG channels\npicks = mne.pick_types(raw.info, meg=True, eeg=False, stim=False, eog=False,\n                       ref_meg=False, exclude='bads')\n\n# Read as a single chunk of data\ny, t = raw.get_data(picks, return_times=True)\nsensor_dim = eelbrain.load.fiff.sensor_dim(raw.info, picks=picks)\ntime = eelbrain.UTS.from_int(0, t.size - 1, raw.info['sfreq'])\nmeg = eelbrain.NDVar(y, dims=(sensor_dim, time))\nprint(meg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Continuous stimulus variable construction\nAfter loading and processing the raw data, we will construct the predictor variable for this particular experiment (by putting an impulse at every event time-point). Note that, the predictor variable and meg response should be of same length. \n\nIn case of repetitive trials (where you will have a :class:`eelbrain.Case` dimension), supply one predictor variable for each trial. Different predictor variables for a single trial can be nested (see :func:`ncrf.fit_ncrf`).\n\nIn this example, we use two different predictor variables for a single trial\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# For the common response, we put impulses at the presentation times of both the audio stimuli (i.e., all beeps).\nstim1 = np.zeros(len(time))\nstim1[events[:, 0]] = 1.\n\n# To distinguish between standard and deviant beeps, we assign 1 and -1 impulses respectively.\nstim2 = stim1.copy()\nstim2[events[np.where(events[:, 2] == 2), 0]] = -1.\nstim1 = eelbrain.NDVar(stim1, time)\nstim2 = eelbrain.NDVar(stim2, time)\n\n# Visualize the stimulus\n# p = eelbrain.plot.LineStack(eelbrain.combine([stim1, stim2]), w=10, h=2.5, legend=False)\np = eelbrain.plot.UTS([stim1, stim2], color='black', stem=True, frame='none', w=10, h=2.5, legend=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Noise covariance estimation\nHere we estimate the noise covariance from empty room data.\nInstead, you can also use pre-stimulus recordings to compute noise covariance.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "noise_path = data_path / 'MEG' / 'bst_auditory' / 'S01_Noise_20131218_01.ds'\nraw_empty_room = mne.io.read_raw_ctf(noise_path, preload=True)\n\n# Apply the same pre-processing steps to empty room data\nraw_empty_room.notch_filter(np.arange(60, 181, 60), fir_design='firwin')\n\nraw_empty_room.filter(1.0, 8.0, fir_design='firwin')\n\nraw_empty_room.resample(100, npad=\"auto\")\n\n# Compute the noise covariance matrix\nnoise_cov = mne.compute_raw_covariance(raw_empty_room, tmin=0, tmax=None, method='shrunk', rank=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Forward model (aka lead-field matrix)\nNow is the time for forward modeling.\n'ico-4' should be sufficient resolution if working with surface source space.\nYou can choose to work with free or constrained lead fields.\n:func`ncrf.fit_ncrf` will choose the appropriate regularizer by looking at the provided lead-field matrix.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# The paths to FreeSurfer reconstructions\nsubjects_dir = data_path / 'subjects'\nsubject = 'bst_auditory'\n\n# mne.viz.plot_bem(subject=subject, subjects_dir=subjects_dir,\n#                  brain_surfaces='white', orientation='coronal')\n\n# The transformation file obtained by coregistration\ntrans = data_path / 'MEG' / 'bst_auditory' / 'bst_auditory-trans.fif'\n\n# Here we look at the head only.\n# mne.viz.plot_alignment(raw.info, trans, subject=subject, dig=True,\n#                        meg=['helmet', 'sensors'], subjects_dir=subjects_dir,\n#                        surfaces='head')\n\nsrcfile = subjects_dir / 'bst_auditory' / 'bem' / 'bst_auditory-ico-4-src.fif'\nif srcfile.is_file():\n    src = mne.read_source_spaces(srcfile)\nelse:\n    src = mne.setup_source_space(subject, spacing='ico4',\n                                 subjects_dir=subjects_dir, add_dist=False)\n    mne.add_source_space_distances(src)\n    mne.write_source_spaces(srcfile, src, overwrite=True)  # needed for smoothing\nsrc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compute the forward solution:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fwdfile = subjects_dir / 'bst_auditory' / 'bem' / 'bst_auditory-ico-4-fwd.fif'\nif fwdfile.is_file():\n    fwd = mne.read_forward_solution(fwdfile)\nelse:\n    conductivity = (0.3,)  # for single layer\n    # conductivity = (0.3, 0.006, 0.3)  # for three layers\n    model = mne.make_bem_model(subject=subject, ico=4,\n                               conductivity=conductivity,\n                               subjects_dir=subjects_dir)\n    bem = mne.make_bem_solution(model)\n\n    fwd = mne.make_forward_solution(raw.info, trans=trans, src=src, bem=bem,\n                                    meg=True, eeg=False, mindist=5.0, n_jobs=2)\n    mne.write_forward_solution(fwdfile, fwd)\n\nfwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Extract the fixed orientation lead field matrix:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fwd_fixed = mne.convert_forward_solution(\n    fwd, surf_ori=True, force_fixed=True, use_cps=True)\n\n# leadfield matrix\nlf = eelbrain.load.fiff.forward_operator(fwd_fixed, src='ico-4', subjects_dir=subjects_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## NCRF estimation\nNow that we have all the required data to estimate NCRFs.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>This example uses simplified settings to speed up estimation:\n\n   1) For this example, we use a fixed regularization parameter (``mu``).\n   For a real experiment, the optimal ``mu`` would be determined by\n   cross-validation (set ``mu='auto'``, which is the default).\n   The optimal ``mu`` will then be stored in ``model.mu``\n   (this is how the ``mu`` used here was determined).\n\n   2) The example forces the estimation to stop after fewer iterations than\n   is recommended (``n_iter``). For stable models, we recommend to use the\n   default setting (``n_iter=10``).</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# To speed up the example, we cache the NCRF:\nncrf_file = data_path / 'MEG' / 'bst_auditory' / 'oddball_ncrf.pickle'\nif ncrf_file.exists():\n    model = eelbrain.load.unpickle(ncrf_file)\nelse:\n    model = fit_ncrf(\n        meg, [stim1, stim2], lf, noise_cov, tstart=0, tstop=0.5,\n        mu=0.0001756774187547859, n_iter=5,\n    )\n    eelbrain.save.pickle(model, ncrf_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The learned kernel/filter (the NCRF) can be accessed as an attribute of the\n``model``.\nNCRFs are stored as :class:`eelbrain.NDVar`. Here, the two NCRFs correspond\nto the two different predictor variables:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model.h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization\nA butterfly plot shows weights in all sources over time.\nThis is good for forming a quick impression of important time lags,\nor peaks in the response:\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Since the estimates are sparse over cortical locations, smoothing the NCRFs over sources to make the visualization more intuitive.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "h = [h.smooth('source', 0.01, 'gaussian') for h in model.h]\np = eelbrain.plot.Butterfly(h)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following code for plotting the anatomical localization\nis commented because the [Mayavi](https://docs.enthought.com/mayavi/mayavi)\nbased plots do not\nwork reliably in the automatic documentation.\nUncomment it to create anatomical plots.\n\nA single time point can be visualized with the PySurfer (:mod:`surfer`)\nbased :func:`eelbrain.plot.brain.brain`:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# brain = eelbrain.plot.brain.brain(h[0].sub(time=0.140), vmax=2e-11, surf='pial')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "An :class:`eelbrain.plot.brain.SequencePlotter` can be used to plot a\nsequence of brain images, for example in a jupyter notebook:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# h_binned = h0.bin(0.1, 0.1, 0.4, 'extrema')\n# sp = eelbrain.plot.brain.SequencePlotter()\n# sp.set_brain_args(surf='inflated')\n# sp.add_ndvar(h_binned)\n# p = sp.plot_table(view='lateral')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In an interactive iPython session, we can also use interactive time-linked\nplots with :func:`eelbrain.plot.brain.butterfly`:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# brain, butterfly = eelbrain.plot.brain.butterfly(h0)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}